{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b881865",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0abd76-60ec-489a-b8f9-e81dab07ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b12e69",
   "metadata": {},
   "source": [
    "Load and preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdcbe01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully!\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK 11 ***\n",
      "\n",
      "[Illustration]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Alice’s Adventures in Wonderland\n",
      "\n",
      "by Lewis Carroll\n",
      "\n",
      "THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "Contents\n",
      "\n",
      " CHAPTER I.     Down the Rabbit-Hole\n",
      " CHAPTER II.    The Pool of Tears\n",
      " CHAPTER III.   A Caucus-Race and a Long Tale\n",
      " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
      " CHAPTER V.     Advice from a Caterpillar\n",
      " CHAPTER VI.    Pig and Pepper\n",
      " CHAPTER VII.   A Mad Tea-Party\n",
      " CHAPTER VIII.  The Queen’s Croquet-Grou\n"
     ]
    }
   ],
   "source": [
    "# URL for the text of \"Alice's Adventures in Wonderland\"\n",
    "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    text = response.text\n",
    "    print(\"Dataset downloaded successfully!\")\n",
    "else:\n",
    "    print(\"Error downloading dataset:\", response.status_code)\n",
    "\n",
    "# Optionally, inspect the first few hundred characters\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740fb6b1",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a49cd7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 51\n",
      "Input shape: (147978, 100)\n",
      "Target shape: (147978, 100)\n"
     ]
    }
   ],
   "source": [
    "# Basic cleaning:\n",
    "# Remove Project Gutenberg header and footer if present.\n",
    "start_marker = r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK .* \\*\\*\\*\"\n",
    "end_marker = r\"\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK .* \\*\\*\\*\"\n",
    "text = re.split(start_marker, text, flags=re.IGNORECASE)[-1]\n",
    "text = re.split(end_marker, text, flags=re.IGNORECASE)[0]\n",
    "\n",
    "# Convert to lowercase and strip extra spaces/newlines\n",
    "text = text.lower().strip()\n",
    "\n",
    "# Basic preprocessing: convert text to lowercase (optional) and remove header/footer\n",
    "#text = text.lower()\n",
    "\n",
    "# Create a set of characters (vocabulary)\n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)\n",
    "print(\"Unique characters:\", vocab_size)\n",
    "\n",
    "# Create mappings from characters to integers and vice versa\n",
    "char_to_idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "# Encode the full text to integers\n",
    "encoded_text = np.array([char_to_idx[ch] for ch in text])\n",
    "\n",
    "# Set sequence length for training examples\n",
    "seq_length = 100\n",
    "\n",
    "# Create input and target sequences using a sliding window approach\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(encoded_text) - seq_length):\n",
    "    inputs.append(encoded_text[i:i+seq_length])\n",
    "    targets.append(encoded_text[i+1:i+seq_length+1])  # target is shifted by one\n",
    "\n",
    "inputs = np.array(inputs)\n",
    "targets = np.array(targets)\n",
    "\n",
    "print(\"Input shape:\", inputs.shape)\n",
    "print(\"Target shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9cedd9",
   "metadata": {},
   "source": [
    "Define the LSTM Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb98bd",
   "metadata": {},
   "source": [
    "Set Hyperparameters and Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cad4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, dropout=0.5, batch_first=True) ## Added dropout for regularization\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        if embed_size == hidden_size:\n",
    "            self.fc.weight = self.embed.weight\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # x: (batch, seq_length)\n",
    "        x = self.embed(x)  # (batch, seq_length, embed_size)\n",
    "        out, hidden = self.lstm(x, hidden)  # out: (batch, seq_length, hidden_size)\n",
    "        # Reshape output for the fully connected layer\n",
    "        out = out.contiguous().view(-1, out.shape[2])\n",
    "        out = self.fc(out)  # (batch * seq_length, vocab_size)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden state and cell state with zeros\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(num_layers, batch_size, hidden_size).zero_(),\n",
    "                  weight.new(num_layers, batch_size, hidden_size).zero_())\n",
    "        return hidden\n",
    "    \n",
    "\n",
    "# Hyperparameters\n",
    "embed_size = 256 ## Set to the same as hidden_size for weight tying\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.002\n",
    "num_epochs = 50 ## Increased epochs for better training\n",
    "batch_size = 256 ## Increased batch size for faster training\n",
    "\n",
    "# Initialize the model, loss function and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CharLSTM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2) ## Added learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149115be",
   "metadata": {},
   "source": [
    "Train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ad31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.9125\n",
      "Epoch [2/50], Loss: 1.5796\n",
      "Epoch [3/50], Loss: 1.4494\n",
      "Epoch [4/50], Loss: 1.3664\n",
      "Epoch [5/50], Loss: 1.3043\n",
      "Epoch [6/50], Loss: 1.2551\n",
      "Epoch [7/50], Loss: 1.2139\n"
     ]
    }
   ],
   "source": [
    "# Utility function to get batches\n",
    "def get_batches(inputs, targets, batch_size):\n",
    "    total_batches = len(inputs) // batch_size\n",
    "    for i in range(0, total_batches * batch_size, batch_size):\n",
    "        x = inputs[i:i+batch_size]\n",
    "        y = targets[i:i+batch_size]\n",
    "        yield torch.LongTensor(x), torch.LongTensor(y)\n",
    "\n",
    "# Training loop\n",
    "all_losses = []\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    epoch_loss = 0.0\n",
    "    batch_count = 0\n",
    "    for x_batch, y_batch in get_batches(inputs, targets, batch_size):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Detach hidden state to prevent backprop through entire training history\n",
    "        hidden = tuple([h.detach() for h in hidden])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs, hidden = model(x_batch, hidden)\n",
    "        \n",
    "        # Reshape target to match outputs\n",
    "        loss = criterion(outputs, y_batch.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  ## Added gradient clipping to prevent exploding gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "    \n",
    "    avg_loss = epoch_loss / batch_count\n",
    "    all_losses.append(avg_loss)\n",
    "    scheduler.step(avg_loss) ## Adjust learning rate based on validation loss\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(all_losses, label='Training Loss')\n",
    "plt.title(\"Training Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f804dac",
   "metadata": {},
   "source": [
    "Evaluate the Model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8368db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "\n",
      "alice first) all voice bringing from the officerfully as well as she evening all. there try\n",
      "thundink three to one of this time, in the dormouse\n",
      "speaking again, the long passing the new was no one great made of the drace. “offence, and it\n",
      "lessons to—the whiter side,” said the king.\n",
      "\n",
      "“call has began o\n"
     ]
    }
   ],
   "source": [
    "def sample(model, start_str, length=200, temperature=1.0): ## Added temperature parameter for sampling\n",
    "    model.eval()\n",
    "    # Convert start string to tensor\n",
    "    input_seq = torch.LongTensor([char_to_idx[ch] for ch in start_str]).unsqueeze(0).to(device)\n",
    "    hidden = model.init_hidden(1)\n",
    "    predicted = start_str\n",
    "    \n",
    "    for _ in range(length):\n",
    "        output, hidden = model(input_seq, hidden)\n",
    "        # Get last character's prediction from the output\n",
    "        output = output[-1] / (temperature if temperature > 0 else 1.0)  ## Added temperature scaling for sampling\n",
    "        prob = torch.softmax(output, dim=0).data\n",
    "        char_idx = torch.multinomial(prob, 1).item()\n",
    "        predicted_char = idx_to_char[char_idx]\n",
    "        predicted += predicted_char\n",
    "        \n",
    "        # Prepare next input\n",
    "        input_seq = torch.LongTensor([[char_idx]]).to(device)\n",
    "    return predicted\n",
    "\n",
    "# Example: generate text starting with \"alice\"\n",
    "seed = \"alice\"\n",
    "generated_text = sample(model, seed, length=300, temperature=0.8)  ## Adjusted temperature for more creative sampling\n",
    "print(\"Generated text:\\n\")\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearningFinalProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
